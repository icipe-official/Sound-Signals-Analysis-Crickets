{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.applications.resnet_v2 import ResNet152V2, preprocess_input\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Flatten, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv data\n",
    "csv_file = \"/media/kanjibrian/Transcend7/concat/concatenated_file.csv\"\n",
    "df_csv = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to load and preprocess the image\n",
    "def load_and_preprocess_images(image_ids, image_folder):\n",
    "    images = []\n",
    "    for image_id in image_ids:\n",
    "        image_path = f\"{image_folder}/{image_id}\"  # Change the extension if images are not in JPG format\n",
    "        img = Image.open(image_path)\n",
    "        img = img.resize((224, 224))  \n",
    "        img = np.array(img)\n",
    "        #remove extra channels\n",
    "        if img.shape[2] > 3:\n",
    "            img=img[:,:,:3]\n",
    "        img = img.astype('float32') / 255.0  # Normalize between 0 and 1\n",
    "        img = preprocess_input(img)  # Preprocess for VGG16\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load and preprocess images\n",
    "image_folder = \"/media/kanjibrian/Transcend7/testing\"\n",
    "image_ids = df_csv['image_id'].values\n",
    "image_data = load_and_preprocess_images(image_ids, image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string labels to numerical labels\n",
    "label_encoder=LabelEncoder()\n",
    "df_csv['Label']=label_encoder.fit_transform(df_csv['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into features (X) and labels (y)\n",
    "X_images=image_data\n",
    "X_features=df_csv[['Temperature','Humidity']].values\n",
    "y=to_categorical(df_csv['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slit data into training and validation sets\n",
    "X_train_img, X_val_img, X_train_feat, X_val_feat, y_train, y_val = train_test_split(X_images, X_features, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load efficientnetB4 model\n",
    "base_model=ResNet152V2(weights='imagenet', include_top= False, input_shape=(224,224,3))\n",
    "#freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "\n",
    "# Create an input layer for the features\n",
    "input_features = Input(shape=(2,))\n",
    "x_feat = Dense(128, activation='relu')(input_features)\n",
    "x_feat= Dense(64, activation='relu')(x_feat)\n",
    "\n",
    "# Concatenate image features and CSV features\n",
    "concatenated=concatenate([x, x_feat])\n",
    "\n",
    "# Final output layer\n",
    "predictions = Dense(len(label_encoder.classes_), activation='softmax')(concatenated)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=[base_model.input, input_features], outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_img, X_train_feat], y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32, \n",
    "    validation_data=([X_val_img, X_val_feat], y_val), \n",
    "    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#extracting loss and validation loss from the history object\n",
    "train_loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "#create a range for the number of epochs\n",
    "epochs=range(1,len(train_loss)+1)\n",
    "#plotting training and validation loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(epochs, train_loss,label='Training loss')\n",
    "plt.plot(epochs,val_loss,label = 'Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "y_pred=model.predict([X_val_img,X_val_feat])\n",
    "y_pred_classes=np.argmax(y_pred,axis=1)\n",
    "y_true=np.argmax(y_val,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get class labels\n",
    "class_labels=label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the metrics\n",
    "accuracy=accuracy_score(y_true, y_pred_classes)\n",
    "f1=f1_score(y_true,y_pred_classes,average='weighted')\n",
    "cm=confusion_matrix(y_true,y_pred_classes,labels=np.arange(len(class_labels)))\n",
    "\n",
    "#print the metrics\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1-Score: \", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, index=class_labels,columns=class_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Deep_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
